{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "qXwZJuRrThrj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "wPSCjsjMTXbN",
        "outputId": "c9b0b932-d6a8-4d55-d21b-0582d9928648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at billfass/my_bert_model and are newly initialized: ['encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'pooler.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.self.query.bias', 'pooler.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7864, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, BertModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"billfass/my_bert_model\")\n",
        "model = BertModel.from_pretrained(\"billfass/my_bert_model\")\n",
        "\n",
        "def predict(text):\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "        output = model(**inputs)\n",
        "\n",
        "        pred = torch.max(output.logits, dim=1)\n",
        "\n",
        "    return {\n",
        "        \"label class\":pred\n",
        "    }\n",
        "\n",
        "\n",
        "demo = gr.Interface(fn=predict, inputs=\"text\", outputs=\"json\")\n",
        "\n",
        "demo.launch()"
      ]
    }
  ]
}